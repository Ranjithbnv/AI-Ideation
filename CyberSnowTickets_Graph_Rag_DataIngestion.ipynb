#installations.
#!pip install ollama
#!pip install python-docx nltk
#!pip install langchain
#!pip install --upgrade --quiet  langchain langchain-community langchain-experimental neo4j
#!pip install langchain_openai
#!pip install json-repair
#!pip install streamlit

from docx import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument

def extract_text_and_tables(doc_path):
    """
    Extracts text and tables from a Word document.
    Returns a single string.
    """
    doc = Document(doc_path)
    extracted_text = []

    # Extract text from paragraphs
    for para in doc.paragraphs:
        if para.text.strip():
            extracted_text.append(para.text.strip())

    # Extract text from tables
    for table in doc.tables:
        for row in table.rows:
            row_text = " | ".join([cell.text.strip() for cell in row.cells])
            extracted_text.append(row_text)

    return "\n".join(extracted_text)

def chunk_text(text,file_path, chunk_size=5120000, overlap=50):   #512
    """
    Splits text into chunks using RecursiveCharacterTextSplitter.
    Returns a list of Langchain Document objects.
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
    chunks = text_splitter.split_text(text)

    # Convert into Langchain Document objects
    documents = [LangchainDocument(page_content=chunk, metadata={
         
            "file_path": file_path,
               
        }) for chunk in chunks]
    return documents

import os

all_doc_chunks = []

def process_all_documents(folder_path):
    for filename in os.listdir(folder_path):
        if filename.endswith(".docx"):
            file_path = os.path.join(folder_path, filename)
            print(f"\nüìÑ Processing: {filename}")
            
            full_text = extract_text_and_tables(file_path)
            doc_chunks = chunk_text(full_text, file_path=file_path)

            if doc_chunks:
              
                all_doc_chunks.extend(doc_chunks)  # Append to the global list
            else:
                print("‚ö†Ô∏è No chunks generated.")



# Set folder path and call function
all_doc_chunks = []
folder_path = "C:\D Drive\Project Documents\Cyber Threat Protection\Tickets for LLM\Set 4 LowSeviarity"  # Replace this with your folder path
process_all_documents(folder_path)

# Optional: print how many chunks you got in total
print(f"\n‚úÖ Total chunks collected: {len(all_doc_chunks)}")
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_openai import AzureChatOpenAI
import os
from dotenv import load_dotenv
from openai import AzureOpenAI


load_dotenv()

endpoint = ""
model_name = "gpt-4o"
deployment = "gpt-4o"

subscription_key = ""
api_version = "2024-12-01-preview"

#llm = AzureOpenAI( api_version=api_version,  azure_endpoint=endpoint,  api_key=subscription_key,)
#response = llm.chat.completions.create( messages=[{"role": "user", "content": "Hello"}], model=model_name,)


from langchain.chat_models import AzureChatOpenAI

llm = AzureChatOpenAI(
    openai_api_key=subscription_key,
    azure_endpoint=endpoint,
    deployment_name=deployment,  # e.g., "gpt-4"
    model_name=model_name,                       # This can be "gpt-4" or "gpt-35-turbo"
    openai_api_version=api_version,  # or the version you deployed
)

response = llm.invoke("Tell me a fun fact about AI.")
print(response.content)



#Define promt template for the LLMGraphTransformer
from langchain.chat_models import ChatOpenAI
from langchain_experimental.graph_transformers.llm import LLMGraphTransformer
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_experimental.graph_transformers.llm import LLMGraphTransformer
from langchain_core.prompts import PromptTemplate
llm = llm


# Define the LLMGraphTransformer with the LLM and prompt template
allowed_nodes = ["Title",
"Alert Urgency",
"Alert Host",
"Alert User",
"Timestamp of Alert",
"Summary",
"Suggested Validation Actions",
"Description / Cause of Concern",
"Sentinel Alert Link",
"Defender Alert Link",
"OSINT Links",
"Analyst Name",
"Reviewer Name",
"Additional Information",


]




graph_transformer = LLMGraphTransformer(
    llm=llm,
    allowed_nodes=allowed_nodes,
    additional_instructions="Add label and title to the nodes."
    
       

)


#Binding the LLM to the Graph Transformer
all_doc_chunks1=all_doc_chunks[0:5] # Limit to first 5 chunks for testing
graph_documents = graph_transformer.convert_to_graph_documents(all_doc_chunks1)

# Convert the documents to graph format

print(f"Nodes:{graph_documents[0].nodes}")
print(f"Relationships:{graph_documents[0].relationships}")
display(graph_documents)
        

#neo4j
from neo4j import GraphDatabase
password = "Graph@123"
# üîÅ Replace these with your actual values
uri = "bolt://localhost:7687"
user = "neo4j"
password = "Graph@123"

# Connect to Neo4j
driver = GraphDatabase.driver(uri, auth=(user, password))

# Test query
def test_connection():
    with driver.session() as session:
        result = session.run("RETURN 'Neo4j is connected!' AS message")
        for record in result:
            print(record["message"])

test_connection()
from langchain_community.graphs import Neo4jGraph
graph = Neo4jGraph(url=uri, username=user, password=password)
AddedDocs= graph.add_graph_documents(
    graph_documents,
    baseEntityLabel=True,
    include_source=True
)
#Adding Embedidngs
from neo4j import GraphDatabase

import numpy as np

from langchain_openai import OpenAIEmbeddings
from langchain_openai import AzureOpenAIEmbeddings

driver = driver
ebed_model="text-embedding-ada-002"  
ebed_azure_endpoint=""
ebed_api_key=""
ebed_openai_api_version="2023-05-15"
embeddings = AzureOpenAIEmbeddings(
    model=ebed_model,    
    azure_endpoint=ebed_azure_endpoint, 
    api_key=ebed_api_key,
    openai_api_version=ebed_openai_api_version
)

def get_all_documents(tx):  # 'tx' is the transaction object passed in
    # query = """
    # MATCH (d:Document)
    # RETURN id(d) AS doc_id, d.text AS text, d.file_path AS file_path
    # """
    query = """
   MATCH (d:Document)
RETURN id(d) AS doc_id, d.text AS text, d.file_path AS file_path
ORDER BY id(d) DESC
LIMIT 4
    """
    result = tx.run(query)  # run your Cypher query inside the transaction
    return [record.data() for record in result]



with driver.session() as session:
    documents = session.read_transaction(get_all_documents)

display(documents)  # Display the keys of the first document
for doc in documents[0:70]:
            print(doc)
            print(f"Document ID: {doc['doc_id']}")
            embedding = embeddings.embed_query(doc["text"])
            docid=doc["doc_id"]
            print(f"Document ID: {docid}")
            graph.query(
    """
    MATCH (d:Document) WHERE ID(d) = $docid
    SET d.embedding_text = $embedding
    RETURN d
    """,
    {"docid": docid, "embedding": embedding}
)
