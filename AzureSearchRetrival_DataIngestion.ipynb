#pip install python-dotenv
#!pip install langchain-openai
#!pip install langchain
#!pip install langchain langchain-community azure-search-documents
#!pip install azure-identity
#!pip install unstructured==0.5.6
#!pip install langchain-community
#!pip install "unstructured[all-docs]"
#!pip install --upgrade certifi
#!pip install python-certifi-win32
#!pip install --upgrade nltk
#!pip install python-docx



#basic Interaction AzureOpenAI
from langchain_openai import AzureChatOpenAI
from dotenv import load_dotenv
from IPython.display import display, Markdown
load_dotenv()

import os
from openai import AzureOpenAI

endpoint = ""
model_name = "gpt-4o"
deployment = "gpt-4o"


api_version = "2024-12-01-preview"


client = AzureOpenAI(
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
)

response = client.chat.completions.create(
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant.",
        },
        {
            "role": "user",
            "content": "I am going to Paris, what should I see?",
        }
    ],
    max_tokens=4096,
    temperature=1.0,
    top_p=1.0,
    model=deployment
)

print(response.choices[0].message.content)







#With langchain

#basic Interaction AzureChatOpenAI
from langchain_openai import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from IPython.display import display, Markdown
load_dotenv()
import os
from openai import AzureOpenAI

def GetCertifications(skills,level):

 templatetext = "Can you help me with 2 certification I can do with my skill: {skills} and level {level}?"
    # Define the prompt template with input variables
 prompt = PromptTemplate(output_key="certification",
        input_variables=["skills", "level"],
        template=templatetext,
    )
 llm = AzureChatOpenAI(
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),   
    model_name="gpt-4o",
  )

 # Chain the prompt and LLM
 chain = prompt | llm
    
 # Invoke the chain with the provided inputs
 response = chain.invoke({"skills": skills, "level": level})
    
 return response





#Knowledge Base Creation
#Test vector
from langchain_openai import AzureOpenAIEmbeddings
from dotenv import load_dotenv
load_dotenv()

from IPython.display import display, Markdown

import os

embeddings = AzureOpenAIEmbeddings(
    model=os.getenv("ebed_model"),    
    azure_endpoint=os.getenv("ebed_azure_endpoint"), 
    api_key=os.getenv("ebed_api_key"),
    openai_api_version=os.getenv("ebed_openai_api_version")
)
input_text = "Incorrect Deployment Name."
vector = embeddings.embed_query(input_text)
print(vector)

#Ingest Data
#Knowledge Base Creation
import os
from langchain_community.vectorstores.azuresearch import AzureSearch
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import UnstructuredURLLoader
from dotenv import load_dotenv
load_dotenv()



vector_store_address: str = os.getenv("AZURE_SEARCH_ENDPOINT")
vector_store_password: str = os.getenv("AZURE_SEARCH_ADMIN_KEY")
index_name: str = "aipocsearchindexv2"
# Initialize the Azure Search vector store
vector_store: AzureSearch = AzureSearch(
    azure_search_endpoint=vector_store_address,
    azure_search_key=vector_store_password,
    index_name=index_name,
    embedding_function=embeddings.embed_query
)

#delete Data from Ai Search

#Knowledge Base Creation
import os
from langchain_community.vectorstores.azuresearch import AzureSearch
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import UnstructuredURLLoader
from dotenv import load_dotenv
load_dotenv()



vector_store_address: str = os.getenv("AZURE_SEARCH_ENDPOINT")
vector_store_password: str = os.getenv("AZURE_SEARCH_ADMIN_KEY")
index_name: str = "aipocsearchindexv2"
# Initialize the Azure Search vector store
vector_store: AzureSearch = AzureSearch(
    azure_search_endpoint=vector_store_address,
    azure_search_key=vector_store_password,
    index_name=index_name,
    embedding_function=embeddings.embed_query
)

#Read Word Documents & Return Chunks

from docx import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from unstructured.partition.docx import partition_docx
from docx import Document
import re

from docx import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document as LangchainDocument

def extract_text_and_tables(doc_path):
    """
    Extracts text and tables from a Word document.
    Returns a single string.
    """
    doc = Document(doc_path)
    extracted_text = []

    # Extract text from paragraphs
    for para in doc.paragraphs:
        if para.text.strip():
            extracted_text.append(para.text.strip())

    # Extract text from tables
    for table in doc.tables:
        for row in table.rows:
            row_text = " | ".join([cell.text.strip() for cell in row.cells])
            extracted_text.append(row_text)

    return "\n".join(extracted_text)

def chunk_text(text, chunk_size=512, overlap=50):
    """
    Splits text into chunks using RecursiveCharacterTextSplitter.
    Returns a list of Langchain Document objects.
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
    chunks = text_splitter.split_text(text)

    # Convert into Langchain Document objects
    documents = [LangchainDocument(page_content=chunk) for chunk in chunks]
    return documents


# Run the extraction and chunking

file_path = "Documents/DCAT Design.docx"
full_text = extract_text_and_tables(file_path)
doc_chunks = chunk_text(full_text)

# Print extracted chunks
#for idx, chunk in enumerate(doc_chunks):
 #   print(f"Chunk {idx+1}:\n{chunk}\n")

print(type(doc_chunks[0])) 


result = vector_store.add_documents(documents=doc_chunks)
print("Documents uploaded successfully!")
print(result)


# do a vector smiliarity search and pass to llm
from langchain_core.prompts import PromptTemplate

query="Get Me list of all API end points"
retrieved_docs = vector_store.similarity_search(query, k=30)


llm = AzureChatOpenAI(
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),   
    model_name="gpt-4o",
  )
context = "\n\n".join([doc.page_content for doc in retrieved_docs])
display(context)

# ðŸ”¹ Step 3: Define a Prompt Template for LLM
prompt_template = PromptTemplate(
    template="You are an AI Intelligent bot who reads technical design docuemnts and answer users questioins in detailed way. you dont have info in context just say you dont have info\n\n"
             "Context:\n{context}\n\n"
             "Question: {query}\n\n"
             "Answer in a concise and detailed manner.",
    input_variables=["context", "query"]
)

# ðŸ”¹ Step 4: Generate a Response Using LLM
formatted_prompt = prompt_template.format(context=context, query=query)
response = llm.invoke(formatted_prompt)

display(Markdown(f"**Chatbot:**\n\n{response.content.strip()}")) # âœ… Multi-line output

#Hybrid Search

from azure.search.documents.models import VectorizableTextQuery
from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType

query = "Whata re different Keys we used in system "
query_embedding = embeddings.embed_query(query)
print(query_embedding)
vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=50, fields="content_vector", exhaustive=True)
print(vector_query)
results = search_client.search(  
    search_text=query,  
    vector_queries=[vector_query],
    select=["id", "content"],
    query_type=QueryType.SEMANTIC,
    semantic_configuration_name='SementicTest',
    query_caption=QueryCaptionType.EXTRACTIVE,
    query_answer=QueryAnswerType.EXTRACTIVE,
    top=50
)



for result in results:
    print( result.keys())
    print(f"Search Score ID: {result['@search.score']}")
    print(f"Search Re Ranker Score: {result['@search.reranker_score']}")
    print(f"Score: {result['@search.score']}")
    print(f"Document ID: {result['id']}")
    print(f"Content: {result['content'][:500]}")  # Show first 500 characters

    print("="*80)
results_list = list(results)  # Convert results to a list
print(f"Total Results: {len(results_list)}")

strong_results = [
            doc for doc in results_list 
            if doc.get('@search.reranker_score') >= 1.5
        ]

print(f"strong_results: {len(strong_results)}")
context = "\n\n".join([doc['content'] for doc in strong_results])


# âœ… Debugging: Check if context is built correctly
print("Extracted Context (First 1000 characters):")
print(context[:1000])  # Print only the first 1000 characters to verify



llm = AzureChatOpenAI(
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),   
    model_name="gpt-4o",
  )

display(context)

# ðŸ”¹ Step 3: Define a Prompt Template for LLM
prompt_template = PromptTemplate(
    template="You are an AI Intelligent bot who reads technical design docuemnts and answer users questioins in detailed way. you dont have info in context just say you dont have info\n\n"
             "Context:\n{context}\n\n"
             "Question: {query}\n\n"
             "Answer in a concise and detailed manner.",
    input_variables=["context", "query"]
)

# ðŸ”¹ Step 4: Generate a Response Using LLM
formatted_prompt = prompt_template.format(context=context, query=query)
response = llm.invoke(formatted_prompt)

display(Markdown(f"**Chatbot:**\n\n{response.content.strip()}")) # âœ… Multi-line output















